{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.11.11' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/home/yoni_ash/.cache/pypoetry/virtualenvs/ml-project-YTCqndst-py3.11/bin/python -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "def is_running_on_colab():\n",
    "    try:\n",
    "        import google.colab\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "def is_not_in_virtualenv():\n",
    "    return hasattr(sys, 'real_prefix') or (hasattr(sys, 'base_prefix') and sys.base_prefix == sys.prefix)\n",
    "\n",
    "def is_first_run():\n",
    "    return not os.path.exists(\"/content/.colab_restarted\")\n",
    "\n",
    "if is_running_on_colab():\n",
    "    if is_first_run():\n",
    "        %pip install torch pandas matplotlib matplotlib opencv-python mediapipe\n",
    "        with open(\"/content/.colab_restarted\", \"w\") as f:\n",
    "            f.write(\"Runtime restarted once\")\n",
    "        print('Restarting ...')\n",
    "        os._exit(00)\n",
    "\n",
    "elif is_not_in_virtualenv():\n",
    "    !poetry config virtualenvs.create false --local\n",
    "    !poetry install --no-root\n",
    "else:\n",
    "    !poetry install --no-root\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-FMQmxefGiE8",
    "outputId": "9c5a0cb5-2599-4db2-d5d4-cfec079c396e"
   },
   "outputs": [],
   "source": [
    "##%%\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Parameters\n",
    "TEST_LIVE = not is_running_on_colab() # Toggle ON if the running environment has access to a camera\n",
    "USE_PRETRAINED_MODEL = True\n",
    "DATA_DIR = './data'\n",
    "MODEL_PATH = f\"{DATA_DIR}/sign_language_99_cnn.pth\"\n",
    "PERFORMANCE_PATH = f\"{DATA_DIR}/training_stats.csv\"\n",
    "MODEL_URL = 'https://drive.google.com/file/d/1aUgeUFy-zpezbFWfMjDtPivatGOv1x5Y/view?usp=drive_link'\n",
    "PERFORMANCE_URL = 'https://drive.google.com/file/d/1vTmCGOLeZtGbM1s7euyz36ejulmmfkIp/view?usp=drive_link'\n",
    "DATA_URL = \"https://www.kaggle.com/api/v1/datasets/download/datamunge/sign-language-mnist\"\n",
    "\n",
    "Learning_Rate = 0.01\n",
    "Epochs = 100\n",
    "confused = np.zeros((26, 26))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\",device)\n",
    "print(\"Setup complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GJ9Hb_WcqRxK"
   },
   "outputs": [],
   "source": [
    "\n",
    "label_list = [chr(i) for i in range(65, 91)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import requests\n",
    "\n",
    "os.makedirs(DATA_DIR,exist_ok=True)\n",
    "ZIP_NAME = f\"{DATA_DIR}/sign-language-mnist.zip\"\n",
    "CSV_FILES = [f\"{DATA_DIR}/sign_mnist_train.csv\", f\"{DATA_DIR}/sign_mnist_test.csv\"]\n",
    "\n",
    "def download_and_extract():\n",
    "    print(\"Downloading dataset...\")\n",
    "    with requests.get(DATA_URL, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(ZIP_NAME, \"wb\") as f:\n",
    "            for chunk in r.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "\n",
    "    print(\"Extracting dataset...\")\n",
    "    with zipfile.ZipFile(ZIP_NAME, 'r') as zip_ref:\n",
    "        zip_ref.extractall(f\"{DATA_DIR}/\")\n",
    "\n",
    "# Check for CSVs and download/extract if needed\n",
    "if not all(os.path.exists(csv) for csv in CSV_FILES):\n",
    "    download_and_extract()\n",
    "else:\n",
    "    print(\"CSV files already present.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-Pw2aSLpo9a_",
    "outputId": "b0b10375-bf51-4322-c111-1d280282fd4e"
   },
   "outputs": [],
   "source": [
    "# This helps speed up data loading time across multiple runs (pkl > csv)\n",
    "def preprocess_and_save(csv_path, pkl_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    labels = df.iloc[:, 0].astype(np.int64).values\n",
    "    images = df.iloc[:, 1:].astype(np.float32).values / 255.0\n",
    "    images = images.reshape((-1, 1, 28, 28))\n",
    "\n",
    "    with open(pkl_path, 'wb') as f:\n",
    "        pickle.dump((images, labels), f)\n",
    "    print(f\"Saved to {pkl_path}\")\n",
    "\n",
    "\n",
    "\n",
    "preprocess_and_save(f\"{DATA_DIR}/sign_mnist_train/sign_mnist_train.csv\", f\"{DATA_DIR}/train.pkl\")\n",
    "preprocess_and_save(f\"{DATA_DIR}/sign_mnist_test/sign_mnist_test.csv\", f\"{DATA_DIR}/test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dL8om77suB5d"
   },
   "outputs": [],
   "source": [
    "##%%\n",
    "class MNIST_Pickle_Dataset(Dataset):\n",
    "    def __init__(self, pkl_file):\n",
    "        with open(pkl_file, 'rb') as f:\n",
    "            self.images, self.labels = pickle.load(f)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.images[idx]), torch.tensor(self.labels[idx])\n",
    "\n",
    "train_Data = MNIST_Pickle_Dataset(f\"{DATA_DIR}/train.pkl\")\n",
    "test_Data = MNIST_Pickle_Dataset(f\"{DATA_DIR}/test.pkl\")\n",
    "\n",
    "loaders = {\n",
    "    'train': DataLoader(train_Data, batch_size=32, shuffle=True),\n",
    "    'test': DataLoader(test_Data, batch_size=32, shuffle=False),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_BFHdM3ouGVX"
   },
   "outputs": [],
   "source": [
    "# The Architecture\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        # Convolutional layers + BatchNorm\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout2d = nn.Dropout2d(0.2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # Adaptive Global Average Pooling to (1,1)\n",
    "        self.gap = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(128, 128)\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        self.fc2 = nn.Linear(128, 26)  # 26 classes for A-Z\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.dropout2d(x)\n",
    "\n",
    "        x = self.pool2(F.relu(self.bn3(self.conv3(x))))\n",
    "        x = self.gap(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)  # No log_softmax here!\n",
    "\n",
    "        return x  # Will use CrossEntropyLoss externally\n",
    "\n",
    "# Setup model, optimizer, loss\n",
    "model = CNN().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=Learning_Rate)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# # Optional scheduler to reduce LR on plateaus\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=4, factor=0.1)\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=25)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "# scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[30, 80], gamma=0.1)\n",
    "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n",
    "# scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ktFQxbLWuIIr"
   },
   "outputs": [],
   "source": [
    "# For saving the model & Resusing it\n",
    "def save_model():\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "    }, MODEL_PATH)\n",
    "    print(f\"Model saved to {MODEL_PATH}\")\n",
    "\n",
    "def load_model():\n",
    "    if os.path.exists(MODEL_PATH):\n",
    "        checkpoint = torch.load(MODEL_PATH, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        print(\"Model loaded.\")\n",
    "    else:\n",
    "        print(\"No saved model found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data, target in loaders['train']:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "\n",
    "        loss = loss_fn(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loaders['train'])\n",
    "\n",
    "def test():\n",
    "    global confused\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    confused.fill(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in loaders['test']:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += loss_fn(output, target).item()\n",
    "            pred = output.argmax(dim=1)\n",
    "            for t, p in zip(target.cpu().numpy(), pred.cpu().numpy()):\n",
    "                confused[t, p] += 1\n",
    "            correct += (pred == target).sum().item()\n",
    "\n",
    "    test_loss /= len(loaders['test'])\n",
    "    accuracy = correct / len(loaders['test'].dataset)\n",
    "    return accuracy, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6rv2AEkmuNw6"
   },
   "outputs": [],
   "source": [
    "# Plot Setup\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_losses = []\n",
    "test_accuracies = []\n",
    "\n",
    "def record_metrics(train_loss, test_accuracy):\n",
    "    train_losses.append(train_loss)\n",
    "    test_accuracies.append(test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "\n",
    "def download_file_from_google_drive(url, destination):\n",
    "    session = requests.Session()\n",
    "\n",
    "    response = session.get(url, stream=True)\n",
    "    file_id = re.findall(r'/d/([^/]+)', url)\n",
    "    if not file_id:\n",
    "        raise ValueError(\"Could not extract file ID from the URL.\")\n",
    "\n",
    "    download_url = f\"https://drive.google.com/uc?export=download&id={file_id[0]}\"\n",
    "    response = session.get(download_url, stream=True)\n",
    "\n",
    "    # Check for confirmation token (for large files)\n",
    "    for key, value in response.cookies.items():\n",
    "        if key.startswith('download_warning'):\n",
    "            download_url = f\"https://drive.google.com/uc?export=download&confirm={value}&id={file_id[0]}\"\n",
    "            response = session.get(download_url, stream=True)\n",
    "            break\n",
    "\n",
    "    with open(destination, \"wb\") as f:\n",
    "        for chunk in response.iter_content(32768):\n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "\n",
    "\n",
    "def ensure_file_downloaded(url, path):\n",
    "    \"\"\"Check if file exists, if not download it\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Downloading {path}...\")\n",
    "        download_file_from_google_drive(url, path)\n",
    "        print(f\"Downloaded {path} successfully!\")\n",
    "    else:\n",
    "        print(f\"{path} already exists, skipping download.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IcJJQU_quQJz",
    "outputId": "020ffeb2-d5d0-4f3e-aec2-51a08020849e"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "if not USE_PRETRAINED_MODEL:\n",
    "    for epoch in range(Epochs):\n",
    "        train_loss = train(epoch)\n",
    "        (test_accuracy,test_loss) = test()\n",
    "        print(f\"Epoch: {epoch}: Avg loss: {test_loss:.4f}, Accuracy: ({test_accuracy*100:.2f}%)\")\n",
    "\n",
    "        record_metrics(train_loss, test_accuracy)\n",
    "        scheduler.step(test_accuracy)\n",
    "        \n",
    "\n",
    "        if test_accuracy >= 99.8:\n",
    "            print(f\" Early stopping: Reached {test_accuracy:.2f}% accuracy at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "    # Save the model after training\n",
    "    performance_df = pd.DataFrame()\n",
    "    performance_df['train_losses'] =  train_losses\n",
    "    performance_df['test_accuracies']= test_accuracies\n",
    "    performance_df.to_csv(PERFORMANCE_PATH, index=False)\n",
    "    save_model()\n",
    "else:\n",
    "\n",
    "    # Load the performance data from the file\n",
    "    ensure_file_downloaded(MODEL_URL, MODEL_PATH)\n",
    "    load_model()\n",
    "    test()\n",
    "\n",
    "    ensure_file_downloaded(PERFORMANCE_URL, PERFORMANCE_PATH)\n",
    "    performance_df = pd.read_csv(PERFORMANCE_PATH)\n",
    "    train_losses = performance_df['train_losses'].astype(float).to_numpy()\n",
    "    test_accuracies = performance_df['test_accuracies'].astype(float).to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "VFYqQTMFvuga",
    "outputId": "19242c51-8a0f-4a2a-fffe-070390ab29f5"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Plot after training\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, len(train_losses)+1), train_losses, label='Train Loss')\n",
    "plt.title(\"Training Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, len(test_accuracies)+1), test_accuracies, label='Test Accuracy', color='green')\n",
    "plt.title(\"Test Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "lZ2LSwNjuQd1",
    "outputId": "6e27099e-4cbc-4039-8e19-bc9a2c686a54"
   },
   "outputs": [],
   "source": [
    "\n",
    "excluded_labels = ['J', 'Z']\n",
    "excluded_indices = [label_list.index(l) for l in excluded_labels]\n",
    "\n",
    "normalized_confusion = confused.astype(np.float32)\n",
    "row_sums = normalized_confusion.sum(axis=1, keepdims=True)\n",
    "normalized_confusion = np.divide(normalized_confusion, row_sums, out=np.zeros_like(normalized_confusion), where=row_sums != 0)\n",
    "\n",
    "# black out excluded rows/cols visually\n",
    "mask = np.ones_like(normalized_confusion)\n",
    "for idx in excluded_indices:\n",
    "    mask[idx, :] = 0\n",
    "    mask[:, idx] = 0\n",
    "\n",
    "\n",
    "display_matrix = np.where(mask == 1, normalized_confusion, -1)\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Custom colormap with black for excluded\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import ListedColormap\n",
    "cmap = cm.get_cmap('coolwarm')\n",
    "new_colors = cmap(np.linspace(0, 1, 256))\n",
    "new_colors[0] = [0, 0, 0, 1]  # RGBa for black\n",
    "custom_cmap = ListedColormap(new_colors)\n",
    "\n",
    "plt.imshow(display_matrix, interpolation='nearest', cmap=custom_cmap, vmin=-1, vmax=1)\n",
    "plt.title('Normalized Confusion Matrix')\n",
    "plt.colorbar()\n",
    "\n",
    "# Draw text only where not excluded\n",
    "for i in range(26):\n",
    "    for j in range(26):\n",
    "        if i in excluded_indices or j in excluded_indices:\n",
    "            continue\n",
    "        count = confused[i, j]\n",
    "        if count >= 0:\n",
    "            plt.text(j, i, f'{int(count)}', ha='center', va='center', color='black', fontsize=9)\n",
    "\n",
    "plt.xticks(np.arange(26), label_list)\n",
    "plt.yticks(np.arange(26), label_list)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w9aO6xZJ01KZ"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "#  crop boxes for letters(left, upper, right, lower)\n",
    "hardcoded_boxes = [\n",
    "    (9, 9, 99, 99),        # A\n",
    "    (119, 9, 209, 99),     # B\n",
    "    (229, 9, 319, 99),     # C\n",
    "    (339, 9, 429, 99),     # D\n",
    "    (449, 9, 539, 99),     # E\n",
    "    (559, 9, 649, 99),     # F\n",
    "\n",
    "    (9, 124, 99, 214),     # G\n",
    "    (119, 124, 209, 214),  # H\n",
    "    (229, 124, 319, 214),  # I\n",
    "    # (339, 124, 429, 214),  # J\n",
    "    (449-90-20, 124, 539-90-20, 214),  # K\n",
    "    (559-90-20, 124, 649-90-20, 214),  # L\n",
    "\n",
    "    (9, 239, 99, 329),     # M\n",
    "    (119-90-20, 239, 209-90-20, 329),  # N\n",
    "    (229-90-20, 239, 319-90-20, 329),  # O\n",
    "    (339-90-20, 239, 429-90-20, 329),  # P\n",
    "    (449-90-20, 239, 539-90-20, 329),  # Q\n",
    "    (559-90-20, 239, 649-90-20, 329),  # R\n",
    "\n",
    "    (9+90*6, 354-90-25, 99+90*6, 444+-90-25),     # S\n",
    "    (119-90-20, 354, 209-90-20, 444),  # T\n",
    "    (229-90-20, 354, 319-90-20, 444),  # U\n",
    "    (339-90-20, 354, 429-90-20, 444),  # V\n",
    "    (449-90-20, 354, 539-90-20, 444),  # W\n",
    "    (559-90-20, 354, 649-90-20, 444),  # X\n",
    "\n",
    "    (665-6-90, 471-6-90-20, 665-6,471-20),       # Y\n",
    "    (665-6-90, 471-6-90-20, 665-6,471-20)      # Z\n",
    "]\n",
    "\n",
    "label_list = [chr(i) for i in range(ord('A'), ord('Z') + 1)]  # ['A', ..., 'Z']\n",
    "\n",
    "\n",
    "large_image_path=f\"{DATA_DIR}/amer_sign2.png\"\n",
    "def get_cropped_image(label):\n",
    "    label_index = label_list.index(label)\n",
    "    box = hardcoded_boxes[label_index]\n",
    "\n",
    "    large_image = Image.open(large_image_path)\n",
    "    cropped_image = large_image.crop(box)\n",
    "    return cropped_image\n",
    "\n",
    "import random\n",
    "\n",
    "def get_misclassified_indexes(target_label, max_test_cases=4):\n",
    "    model.eval()\n",
    "    misclassified = []\n",
    "\n",
    "    target_index = label_list.index(target_label)\n",
    "\n",
    "    for idx, (data, target) in enumerate(test_Data):\n",
    "        if target.item() != target_index:\n",
    "            continue  # Only check samples with target label == target_label\n",
    "\n",
    "        data = data.unsqueeze(0).to(device)\n",
    "        output = model(data)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "\n",
    "        if pred.item() != target_index:\n",
    "            misclassified.append(idx)\n",
    "\n",
    "        if len(misclassified) >= max_test_cases:\n",
    "            break  # Limit to max_test_cases\n",
    "\n",
    "    return misclassified\n",
    "\n",
    "def get_random_correctly_classified_indexes(target_label, max_test_cases=4):\n",
    "    model.eval()\n",
    "    correctly_classified = []\n",
    "\n",
    "    target_index = label_list.index(target_label)\n",
    "\n",
    "    # Track how many correctly classified cases we've found\n",
    "    correct_count = 0\n",
    "\n",
    "    while correct_count < max_test_cases:\n",
    "        idx = random.randint(0, len(test_Data) - 1)  # Randomly select an index\n",
    "        data, target = test_Data[idx]\n",
    "\n",
    "        if target.item() == target_index:  # Only consider if the label matches target\n",
    "            data = data.unsqueeze(0).to(device)\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "\n",
    "            if pred.item() == target_index:  # Check if it was classified correctly\n",
    "                correctly_classified.append(idx)\n",
    "                correct_count += 1  # Increment count if classified correctly\n",
    "\n",
    "    return correctly_classified\n",
    "\n",
    "\n",
    "# Function to display multiple test images and their cropped reference images\n",
    "def display_images(indexes):\n",
    "    counter =0\n",
    "    for index in indexes:\n",
    "        model.eval()\n",
    "        data, target = test_Data[index]\n",
    "        data = data.unsqueeze(0).to(device)\n",
    "        output = model(data)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        image = data.squeeze().cpu().numpy()\n",
    "\n",
    "        true_label = label_list[target.item()]\n",
    "        pred_label = label_list[pred.item()]\n",
    "\n",
    "        # Decide how many images to show based on whether prediction was wrong\n",
    "        show_pred_ref = pred_label != true_label\n",
    "        num_cols = 3 if show_pred_ref else 2\n",
    "\n",
    "        plt.figure(figsize=(4 * num_cols, 4))\n",
    "\n",
    "        # Test image\n",
    "        plt.subplot(1, num_cols, 1)\n",
    "        plt.imshow(image, cmap='gray')\n",
    "        plt.title(f'True: {true_label} | Pred: {pred_label}')\n",
    "        plt.axis('off')\n",
    "\n",
    "        # True label ref image\n",
    "        cropped_true = get_cropped_image(true_label)\n",
    "        plt.subplot(1, num_cols, 2)\n",
    "        plt.imshow(cropped_true)\n",
    "        plt.title(f'True Label: {true_label}')\n",
    "        plt.axis('off')\n",
    "\n",
    "        # Predicted label ref image (only if different)\n",
    "        if show_pred_ref:\n",
    "            cropped_pred = get_cropped_image(pred_label)\n",
    "            plt.subplot(1, num_cols, 3)\n",
    "            plt.imshow(cropped_pred)\n",
    "            plt.title(f'Predicted Label: {pred_label}')\n",
    "            plt.axis('off')\n",
    "\n",
    "        counter+=1\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    print(f\"Showing {counter} test cases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SKUH0VYU1xpX"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generate_random_indexes(N):\n",
    "    return random.sample(range(7170), N)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Vtmt6Ea71Hmw",
    "outputId": "b6e55a5b-4a5f-4f22-b20d-0720699e5a8f"
   },
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# Initial option selector\n",
    "option_selector = widgets.Dropdown(\n",
    "    options=['Random', 'Preset', 'Misclassified', 'Correctly Classified'],\n",
    "    value='Preset',  # Default is 'Preset'\n",
    "    description='Mode:'\n",
    ")\n",
    "\n",
    "# Misclassified letter selector, initially hidden\n",
    "letter_selector = widgets.Dropdown(\n",
    "    options=[chr(i) for i in range(ord('A'), ord('Z')) if chr(i) != 'Z'],\n",
    "    description='Letter:',\n",
    "    disabled=False,  # Initially enabled but won't show unless Misclassified is selected\n",
    "    value='E'\n",
    ")\n",
    "\n",
    "# Place the two selectors side by side using HBox\n",
    "selectors_box = widgets.HBox([option_selector, letter_selector])\n",
    "\n",
    "# Function to update images based on the selection\n",
    "def on_option_change(change):\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        run_display(change['new'])\n",
    "\n",
    "# Function to update images based on the selected letter (Misclassified mode)\n",
    "def on_letter_change(change):\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        run_display(option_selector.value)\n",
    "\n",
    "# Function to handle the display and clearing\n",
    "def run_display(mode):\n",
    "    clear_output(wait=True)\n",
    "    display(selectors_box)\n",
    "    \n",
    "    # Show letter selector only when 'Misclassified' is selected\n",
    "    if mode == 'Misclassified' or mode == 'Correctly Classified':\n",
    "        letter_selector.layout.display = 'inline-block'  # Show the letter selector\n",
    "    else:\n",
    "        letter_selector.layout.display = 'none'  # Hide the letter selector\n",
    "    \n",
    "    # Update the display based on the selected mode\n",
    "    if mode == 'Random':\n",
    "        indexes_to_test = generate_random_indexes(4)  # Fixed to 4 for Random mode\n",
    "    elif mode == 'Preset':\n",
    "        indexes_to_test = [1, 15, 7000, 61]\n",
    "    elif mode == 'Misclassified' and letter_selector.value:\n",
    "        indexes_to_test = get_misclassified_indexes(letter_selector.value)  # Get misclassified for selected letter\n",
    "    elif mode == 'Correctly Classified' and letter_selector.value:\n",
    "        indexes_to_test = get_random_correctly_classified_indexes(letter_selector.value)  # Get correctly classified for selected letter\n",
    "    display_images(indexes_to_test)\n",
    "\n",
    "# Observing the change in selection for both option and letter selector\n",
    "option_selector.observe(on_option_change)\n",
    "letter_selector.observe(on_letter_change)\n",
    "\n",
    "# Initial run with default 'Preset' mode\n",
    "run_display(option_selector.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import torch\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "def run_live_demo():\n",
    "    # Configuration parameters\n",
    "    HAND_SELECTOR_SENSITIVITY = 1.8  # Higher = larger hand selection area\n",
    "    MIN_DETECTION_CONFIDENCE = 0.5   # Hand detection sensitivity (0-1)\n",
    "    MIN_TRACKING_CONFIDENCE = 0.5     # Hand tracking sensitivity (0-1)\n",
    "    LATERAL_INVERT = False            # Set to True to mirror the image\n",
    "\n",
    "    # Initialize MediaPipe Hands with configurable sensitivity\n",
    "    mp_hands = mp.solutions.hands\n",
    "    hands = mp_hands.Hands(\n",
    "        static_image_mode=False,\n",
    "        max_num_hands=2,\n",
    "        min_detection_confidence=MIN_DETECTION_CONFIDENCE,\n",
    "        min_tracking_confidence=MIN_TRACKING_CONFIDENCE\n",
    "    )\n",
    "\n",
    "    # English alphabet letters\n",
    "    LETTERS = list(string.ascii_lowercase)\n",
    "\n",
    "    # Simple model for demonstration\n",
    "    class HandModel(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.conv1 = torch.nn.Conv2d(1, 10, kernel_size=3)\n",
    "            self.fc = torch.nn.Linear(10*26*26, 26)\n",
    "            \n",
    "        def forward(self, x):\n",
    "            x = torch.relu(self.conv1(x))\n",
    "            x = x.view(x.size(0), -1)\n",
    "            return torch.softmax(self.fc(x), dim=1)\n",
    "\n",
    "    model = HandModel()\n",
    "    model.eval()\n",
    "\n",
    "    try:\n",
    "        cap = cv2.VideoCapture(0)\n",
    "        active_windows = set()\n",
    "        main_window_open = True\n",
    "\n",
    "        while cap.isOpened() and main_window_open:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                continue\n",
    "            \n",
    "            # Lateral inversion if enabled\n",
    "            if LATERAL_INVERT:\n",
    "                frame = cv2.flip(frame, 1)\n",
    "            \n",
    "            # Process frame\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            results = hands.process(frame_rgb)\n",
    "            \n",
    "            display_frame = frame.copy()\n",
    "            current_hands = set()\n",
    "            hand_images = []\n",
    "            \n",
    "            if results.multi_hand_landmarks:\n",
    "                for i, hand_landmarks in enumerate(results.multi_hand_landmarks):\n",
    "                    # Get hand bounding box with adjustable sensitivity\n",
    "                    landmarks = np.array([(lm.x * frame.shape[1], lm.y * frame.shape[0]) \n",
    "                                        for lm in hand_landmarks.landmark])\n",
    "                    x_min, y_min = landmarks.min(axis=0)\n",
    "                    x_max, y_max = landmarks.max(axis=0)\n",
    "                    \n",
    "                    # Make square region with configurable padding\n",
    "                    side = max(x_max - x_min, y_max - y_min) * HAND_SELECTOR_SENSITIVITY\n",
    "                    center_x, center_y = (x_min + x_max)/2, (y_min + y_max)/2\n",
    "                    x_min, x_max = int(center_x - side/2), int(center_x + side/2)\n",
    "                    y_min, y_max = int(center_y - side/2), int(center_y + side/2)\n",
    "                    \n",
    "                    # Clip coordinates\n",
    "                    x_min, y_min = max(0, x_min), max(0, y_min)\n",
    "                    x_max, y_max = min(frame.shape[1], x_max), min(frame.shape[0], y_max)\n",
    "                    \n",
    "                    # Extract and process hand region\n",
    "                    hand_region = frame[y_min:y_max, x_min:x_max]\n",
    "                    if hand_region.size > 0:\n",
    "                        gray = cv2.cvtColor(hand_region, cv2.COLOR_BGR2GRAY)\n",
    "                        resized = cv2.resize(gray, (28, 28), interpolation=cv2.INTER_AREA)\n",
    "                        hand_images.append((i, resized))\n",
    "                        \n",
    "                        # Draw bounding box with hand ID\n",
    "                        cv2.rectangle(display_frame, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
    "                        cv2.putText(display_frame, f\"Hand {i}\", (x_min, y_min - 10), \n",
    "                                   cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "                        current_hands.add(i)\n",
    "            \n",
    "            # Close windows for hands no longer visible\n",
    "            for window_id in list(active_windows):\n",
    "                if window_id not in current_hands:\n",
    "                    cv2.destroyWindow(f\"Hand {window_id}\")\n",
    "                    active_windows.remove(window_id)\n",
    "            \n",
    "            # Process through model if hands detected\n",
    "            if hand_images:\n",
    "                # Prepare tensor [batch, channel, height, width]\n",
    "                images = [img for (i, img) in hand_images]\n",
    "                hand_tensors = torch.stack([\n",
    "                    torch.from_numpy(img).float().unsqueeze(0)/255.0 \n",
    "                    for img in images\n",
    "                ])\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    outputs = model(hand_tensors)\n",
    "                \n",
    "                # Display hand crops with predictions\n",
    "                for (i, img), output in zip(hand_images, outputs):\n",
    "                    # Get top prediction\n",
    "                    probs, indices = torch.topk(output, 3)\n",
    "                    top_letters = [LETTERS[idx] for idx in indices]\n",
    "                    \n",
    "                    # Create display image\n",
    "                    display_img = cv2.resize(img, (280, 280), interpolation=cv2.INTER_NEAREST)\n",
    "                    display_img = cv2.cvtColor(display_img, cv2.COLOR_GRAY2BGR)\n",
    "                    \n",
    "                    # Display top 3 predictions\n",
    "                    for j, (letter, prob) in enumerate(zip(top_letters, probs)):\n",
    "                        y_pos = 30 + j * 30\n",
    "                        cv2.putText(display_img, f\"{letter}: {prob:.2f}\", \n",
    "                                    (10, y_pos), cv2.FONT_HERSHEY_SIMPLEX, 0.8, \n",
    "                                    (0, 0, 255), 2)\n",
    "                    \n",
    "                    # Create or update hand window\n",
    "                    if i not in active_windows:\n",
    "                        cv2.namedWindow(f\"Hand {i}\", cv2.WINDOW_AUTOSIZE)\n",
    "                        active_windows.add(i)\n",
    "                    \n",
    "                    cv2.imshow(f\"Hand {i}\", display_img)\n",
    "            \n",
    "            # Display main view\n",
    "            cv2.imshow(\"Hand Tracking\", display_frame)\n",
    "            \n",
    "            # Check for window close events or 'q' press\n",
    "            key = cv2.waitKey(1)\n",
    "            if key & 0xFF == ord('q'):\n",
    "                break\n",
    "                \n",
    "            # Check if main window was closed\n",
    "            if cv2.getWindowProperty(\"Hand Tracking\", cv2.WND_PROP_VISIBLE) < 1:\n",
    "                main_window_open = False\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {str(e)}\")\n",
    "\n",
    "    finally:\n",
    "        # Clean up all windows\n",
    "        for window_id in active_windows:\n",
    "            cv2.destroyWindow(f\"Hand {window_id}\")\n",
    "        cv2.destroyAllWindows()\n",
    "        if 'cap' in locals():\n",
    "            cap.release()\n",
    "        if 'hands' in locals():\n",
    "            hands.close()\n",
    "        print(\"Resources released successfully\")\n",
    "if TEST_LIVE: run_live_demo()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
